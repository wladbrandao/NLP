{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Processing.ipynb","provenance":[],"collapsed_sections":["-FOA_3P1gqH-","oR2UoakMkWJ7","SX7OJTv6g3wW","Tf_dAub8lO97","gUhPxgeZvEzK","qxmbkOqhzomk","LTiHsQtzt4wo","yg5LTKV6vrFL","Vec9SGX2w9Yq","uycW_J1Ayj54","HqbDYr_T2leF","WZJpWkgtMIh9","yBOMcgMc29og"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0yhIN_8UySoa","colab_type":"text"},"source":["# LICENSE NOTES\n","---\n","This material was develop by [Prof. Wladmir Cardoso Brandão](http://www.wladmirbrandao.com) and is distributed under the [Creative Commons Attribution-NonCommercial License (CC BY-NC)](https://creativecommons.org/licenses/by-nc/4.0/), what means that anyone can share and adapt the material for non-commercial purposes, since acknowledging it but with no license to derivative it on the same terms."]},{"cell_type":"markdown","metadata":{"id":"SspcGgM3qTgI","colab_type":"text"},"source":["# Text Processing\n","---\n","\n","There are several problems that must be addressed to process text, such as symbol segmentation and removal, and term transformation. In this notebook, one present  approaches to address the most usual problems. [Documents](https://en.wikipedia.org/wiki/Document) are information items that represent thoughts. Text documents are usually composed by sentences, i.e., logically linked sequence of [words (or terms)](https://en.wikipedia.org/wiki/Word) and punctuation characters. "]},{"cell_type":"code","metadata":{"id":"wvVoeA2fgE07","colab_type":"code","colab":{}},"source":["try:\n","    import nltk    \n","except:\n","    !pip install -U nltk\n","    import nltk\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OU1h8O5RzwP-","colab_type":"code","outputId":"e688bf11-7a5b-4761-feff-1028288ab767","executionInfo":{"status":"ok","timestamp":1584100637021,"user_tz":180,"elapsed":726,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from nltk import word_tokenize\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Zg6YZzL_wRsE","colab_type":"code","colab":{}},"source":["text = \"Men, plans, an astonishing canal - Panama.\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-eJWkEnIgFf_","colab_type":"text"},"source":["## Tokenization\n","[Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of demarcating sections (tokens) of a string. Particullarly, a tokenizer is a [text segmentation](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation) approach that divides a string into its component tokens."]},{"cell_type":"code","metadata":{"id":"AyBs9FAcgWLb","colab_type":"code","outputId":"fbd2a736-204d-4e79-c33e-0dcf10c46869","executionInfo":{"status":"ok","timestamp":1584100642482,"user_tz":180,"elapsed":704,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tokens = word_tokenize(text)\n","tokens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Men', ',', 'plans', ',', 'an', 'astonishing', 'canal', '-', 'Panama', '.']"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"StRBiIRQgUZT","colab_type":"text"},"source":["## Normalization\n","[Token Normalization](https://nlp.stanford.edu/IR-book/html/htmledition/normalization-equivalence-classing-of-terms-1.html) is the process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens. For instance, the tokens *\"USA\"* and *\"U.S.A\"* shoud be canonized to *\"usa\"*. Token normalization includes lower case reduction, accent and diacritic removal, and canonicalizing of acronyms, currency, date and hyphenated words."]},{"cell_type":"code","metadata":{"id":"go8g40RrgrxO","colab_type":"code","outputId":"74b16f94-0757-4354-c93d-38c90061ae70","executionInfo":{"status":"ok","timestamp":1584100645131,"user_tz":180,"elapsed":757,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tokens = [token.lower() for token in tokens]\n","tokens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['men', ',', 'plans', ',', 'an', 'astonishing', 'canal', '-', 'panama', '.']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"-FOA_3P1gqH-","colab_type":"text"},"source":["## Filtering\n","Token Filtering is the process to remove unusefull tokens, such as punctuation and special characters."]},{"cell_type":"code","metadata":{"id":"qGERYNdHg4z1","colab_type":"code","outputId":"de416f1f-b20c-43f1-8cce-8e969e24659a","executionInfo":{"status":"ok","timestamp":1584100647443,"user_tz":180,"elapsed":705,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["regex = r\"(?<!\\d)[.,;:-](?!\\d)\"\n","tokens = [re.sub(regex, \"\", token, 0) for token in tokens]\n","tokens = filter(None, tokens)\n","tokens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<filter at 0x7f46065ca0f0>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"oR2UoakMkWJ7","colab_type":"text"},"source":["## Lemmatization\n","\n","---\n","\n","[Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) is the process of removing inflectional endings to return the base or dictionary form of a word, the *lemma*, by using of a vocabulary and morphological analysis of words."]},{"cell_type":"code","metadata":{"id":"RbiaCzPklQPW","colab_type":"code","outputId":"8434471c-0c1c-4219-9692-37b06b61ebdc","executionInfo":{"status":"ok","timestamp":1584100650544,"user_tz":180,"elapsed":903,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from nltk.stem import WordNetLemmatizer\n","nltk.download(\"wordnet\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"EhVp_wUSwLwH","colab_type":"code","outputId":"7789785c-c66f-418f-b6c2-b4a2812de2b7","executionInfo":{"status":"ok","timestamp":1584100652412,"user_tz":180,"elapsed":701,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lemmatizer = WordNetLemmatizer()\n","tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","tokens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['men', 'plan', 'an', 'astonishing', 'canal', 'panama']"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"SX7OJTv6g3wW","colab_type":"text"},"source":["## Stemming\n","[Stemming](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) is the process of removing derivational affixes to return the root form of a word, the *stem*. The most popular stemming approaches perform [suffix stripping](https://dl.acm.org/citation.cfm?id=275537.275705) on words."]},{"cell_type":"code","metadata":{"id":"JQV7Mn23kfPr","colab_type":"code","colab":{}},"source":["from nltk.stem.porter import PorterStemmer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQYr-MpAwJtG","colab_type":"code","outputId":"727d8067-8291-4aa5-ea11-644651468e57","executionInfo":{"status":"ok","timestamp":1584100656972,"user_tz":180,"elapsed":635,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["stemmer = PorterStemmer()\n","tokens = [stemmer.stem(token) for token in tokens]\n","tokens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['men', 'plan', 'an', 'astonish', 'canal', 'panama']"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"Tf_dAub8lO97","colab_type":"text"},"source":["## Stop-Words\n","[Stop-words](https://en.wikipedia.org/wiki/Stop_words) are some extremely common and meaningless words which would appear to be of little value to express the topic of a document. The [general strategy for determining a stop list](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) is to sort the terms by corpus frequency (the total number of times each term appears in the corpus), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents. "]},{"cell_type":"code","metadata":{"id":"9Qo10NoLlqha","colab_type":"code","outputId":"f1e220dc-eb2f-435d-8de5-b858c4290bab","executionInfo":{"status":"ok","timestamp":1584100659575,"user_tz":180,"elapsed":658,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import wordpunct_tokenize"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ACJtFP7DwGXC","colab_type":"code","outputId":"b353e942-8f87-42c6-b1af-a749d770c58e","executionInfo":{"status":"ok","timestamp":1584100662384,"user_tz":180,"elapsed":651,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tokens = [token for token in tokens if token not in stopwords.words('english')]\n","tokens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['men', 'plan', 'astonish', 'canal', 'panama']"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"W6WysMRnHng5","colab_type":"text"},"source":["## Tagging"]},{"cell_type":"markdown","metadata":{"id":"JprV3VnDkxX3","colab_type":"text"},"source":["Tagging, also known as [part-of-speech tagging (POST)](https://en.wikipedia.org/wiki/Part-of-speech_tagging), is the process of marking up a word in a text as corresponding to a particular category that have similar grammatical properties, such as noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, numeral, article, or determiner. The most [popular taggers](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python) use supervised learning approaches trained in well known corpora to classify words in grammatical classes."]},{"cell_type":"code","metadata":{"id":"gfFxe9KBHrHp","colab_type":"code","outputId":"3ff019fc-12b3-44bb-ab21-5ccf67f22a09","executionInfo":{"status":"ok","timestamp":1584100666060,"user_tz":180,"elapsed":694,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["nltk.download('averaged_perceptron_tagger')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"Dt8Sygh_wCRI","colab_type":"code","outputId":"8fcba8e6-d465-4de9-9569-0a98fa3b91ac","executionInfo":{"status":"ok","timestamp":1584100669003,"user_tz":180,"elapsed":647,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["tagged = nltk.pos_tag(tokens)\n","tagged[0:100]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('men', 'NNS'),\n"," ('plan', 'VBP'),\n"," ('astonish', 'JJ'),\n"," ('canal', 'JJ'),\n"," ('panama', 'NN')]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"XZfhAqJlHWGe","colab_type":"text"},"source":["## Chunking"]},{"cell_type":"markdown","metadata":{"id":"4EasTzKNmy4u","colab_type":"text"},"source":["Chunking, also known as [shallow parsing](https://en.wikipedia.org/wiki/Shallow_parsing), performs sentence analysis based on tagging to add more structure to the sentence by grouping words in chunks, higher order units that have discrete grammatical meanings, such as noun groups or phrases and verb groups. Popular chunkers also use supervised learning classifiers to link words and provide chunks."]},{"cell_type":"code","metadata":{"id":"gHvUr7kvHZeI","colab_type":"code","outputId":"8e89b7f9-ab2b-4038-f03c-e393114491a5","executionInfo":{"status":"ok","timestamp":1584100673058,"user_tz":180,"elapsed":655,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"lKwsZuoZv6NC","colab_type":"code","outputId":"d4806cf2-e624-4113-bf0f-0eba26a375a4","executionInfo":{"status":"ok","timestamp":1584100837638,"user_tz":180,"elapsed":667,"user":{"displayName":"Wladmir Cardoso Brandão","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_7mPD5_9M7MhGFamxsQ6rE3ZHYHg3GKEs6Dm2d4I=s64","userId":"11461134605804345265"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["entities = nltk.chunk.ne_chunk(tagged)\n","print(entities)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(S men/NNS plan/VBP astonish/JJ canal/JJ panama/NN)\n"],"name":"stdout"}]}]}